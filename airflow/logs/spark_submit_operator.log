*** Reading local file: /home/tia/airflow/logs/dag_id=my_app/run_id=manual__2022-05-26T20:52:31.757641+00:00/task_id=app/attempt=1.log
[2022-05-26 23:52:33,830] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: my_app.app manual__2022-05-26T20:52:31.757641+00:00 [queued]>
[2022-05-26 23:52:33,835] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: my_app.app manual__2022-05-26T20:52:31.757641+00:00 [queued]>
[2022-05-26 23:52:33,835] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-05-26 23:52:33,835] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2022-05-26 23:52:33,836] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-05-26 23:52:33,846] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): app> on 2022-05-26 20:52:31.757641+00:00
[2022-05-26 23:52:33,855] {standard_task_runner.py:52} INFO - Started process 21311 to run task
[2022-05-26 23:52:33,859] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'my_app', 'app', 'manual__2022-05-26T20:52:31.757641+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmpb75n0fh9', '--error-file', '/tmp/tmpqa9r306j']
[2022-05-26 23:52:33,860] {standard_task_runner.py:80} INFO - Job 46: Subtask app
[2022-05-26 23:52:33,891] {task_command.py:369} INFO - Running <TaskInstance: my_app.app manual__2022-05-26T20:52:31.757641+00:00 [running]> on host ZHUKAUAT15.itd.iba.by
[2022-05-26 23:52:33,922] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=my_app
AIRFLOW_CTX_TASK_ID=app
AIRFLOW_CTX_EXECUTION_DATE=2022-05-26T20:52:31.757641+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-05-26T20:52:31.757641+00:00
[2022-05-26 23:52:33,926] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2022-05-26 23:52:33,927] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master local --conf spark.executor.instances=2 --conf spark.executor.memory=1000m --jars /mnt/c/Users/user/Desktop/Scala_training/jars/* --name spark-app --class Main --verbose --queue root.default /mnt/c/Users/user/Desktop/Scala_training/jars/app.jar -m db2 -a read -n 5
[2022-05-26 23:52:35,007] {spark_submit.py:488} INFO - Using properties file: null
[2022-05-26 23:52:35,081] {spark_submit.py:488} INFO - 22/05/26 23:52:35 WARN Utils: Your hostname, ZHUKAUAT15 resolves to a loopback address: 127.0.1.1; using 172.31.192.1 instead (on interface eth3)
[2022-05-26 23:52:35,081] {spark_submit.py:488} INFO - 22/05/26 23:52:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2022-05-26 23:52:35,118] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-05-26 23:52:35,118] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/tia/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-05-26 23:52:35,118] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-05-26 23:52:35,118] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-05-26 23:52:35,118] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - Parsed arguments:
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - master                  local
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - deployMode              null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - executorMemory          1000m
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - executorCores           null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - totalExecutorCores      null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - propertiesFile          null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - driverMemory            null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - driverCores             null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - driverExtraClassPath    null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - driverExtraLibraryPath  null
[2022-05-26 23:52:35,156] {spark_submit.py:488} INFO - driverExtraJavaOptions  null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - supervise               false
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - queue                   root.default
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - numExecutors            2
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - files                   null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - pyFiles                 null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - archives                null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - mainClass               Main
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - primaryResource         file:/mnt/c/Users/user/Desktop/Scala_training/jars/app.jar
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - name                    spark-app
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - childArgs               [-m db2 -a read -n 5]
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - jars                    file:/mnt/c/Users/user/Desktop/Scala_training/jars/*
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - packages                null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - packagesExclusions      null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - repositories            null
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - verbose                 true
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - 
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - Spark properties used, including those specified through
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - --conf and those from the properties file null:
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - (spark.executor.instances,2)
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - (spark.executor.memory,1000m)
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - 
[2022-05-26 23:52:35,157] {spark_submit.py:488} INFO - 
[2022-05-26 23:52:35,459] {spark_submit.py:488} INFO - 22/05/26 23:52:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - Main class:
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - Main
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - Arguments:
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - -m
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - db2
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - -a
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - read
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - -n
[2022-05-26 23:52:35,651] {spark_submit.py:488} INFO - 5
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - Spark config:
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.jars,file:/mnt/c/Users/user/Desktop/Scala_training/jars/app.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/aws-java-sdk-core-1.12.187.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/aws-java-sdk-s3-1.12.187.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/jcc-11.5.7.0.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/stocator-1.1.4.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/app.jar)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.app.name,spark-app)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.executor.instances,2)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.submit.pyFiles,)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.submit.deployMode,client)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.master,local)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.executor.memory,1000m)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - (spark.repl.local.jars,file:/mnt/c/Users/user/Desktop/Scala_training/jars/app.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/aws-java-sdk-core-1.12.187.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/aws-java-sdk-s3-1.12.187.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/jcc-11.5.7.0.jar,file:/mnt/c/Users/user/Desktop/Scala_training/jars/stocator-1.1.4.jar)
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - Classpath elements:
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - file:/mnt/c/Users/user/Desktop/Scala_training/jars/app.jar
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - file:/mnt/c/Users/user/Desktop/Scala_training/jars/app.jar
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - file:/mnt/c/Users/user/Desktop/Scala_training/jars/aws-java-sdk-core-1.12.187.jar
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - file:/mnt/c/Users/user/Desktop/Scala_training/jars/aws-java-sdk-s3-1.12.187.jar
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - file:/mnt/c/Users/user/Desktop/Scala_training/jars/jcc-11.5.7.0.jar
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - file:/mnt/c/Users/user/Desktop/Scala_training/jars/stocator-1.1.4.jar
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - 
[2022-05-26 23:52:35,655] {spark_submit.py:488} INFO - 
[2022-05-26 23:52:35,684] {spark_submit.py:488} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-05-26 23:52:39,084] {spark_submit.py:488} INFO - 22/05/26 23:52:39 INFO DB2Connection: Reading table: ARSENI_SALES_DATA
[2022-05-26 23:52:45,048] {spark_submit.py:488} INFO - +----------+-------------+----+------+
[2022-05-26 23:52:45,048] {spark_submit.py:488} INFO - |product_id|product_group|year| sales|
[2022-05-26 23:52:45,048] {spark_submit.py:488} INFO - +----------+-------------+----+------+
[2022-05-26 23:52:45,048] {spark_submit.py:488} INFO - |         0|            0|2015|787756|
[2022-05-26 23:52:45,048] {spark_submit.py:488} INFO - |         8|            1|2015|586892|
[2022-05-26 23:52:45,048] {spark_submit.py:488} INFO - |         1|            4|2015|574870|
[2022-05-26 23:52:45,049] {spark_submit.py:488} INFO - |         3|            6|2015|627011|
[2022-05-26 23:52:45,049] {spark_submit.py:488} INFO - |        10|            3|2016|602343|
[2022-05-26 23:52:45,049] {spark_submit.py:488} INFO - +----------+-------------+----+------+
[2022-05-26 23:52:45,049] {spark_submit.py:488} INFO - 
[2022-05-26 23:52:45,228] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=my_app, task_id=app, execution_date=20220526T205231, start_date=20220526T205233, end_date=20220526T205245
[2022-05-26 23:52:45,269] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-05-26 23:52:45,289] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check

